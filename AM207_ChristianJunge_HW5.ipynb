{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AM 207**: Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verena Kaynig-Fittkau and Pavlos Protopapas  <br>\n",
    "**Due: 11.59 P.M. Thursday April 14th, 2016**\n",
    "\n",
    "### Note: This homework is only for one week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "+ Upload your answers in an ipython notebook to Canvas.\n",
    "\n",
    "+ We will provide you imports for your ipython notebook. Please do not import additional libraries.\n",
    "\n",
    "+ Your individual submissions should use the following filenames: AM207_YOURNAME_HW5.ipynb\n",
    "\n",
    "+ Your code should be in code cells as part of your ipython notebook. Do not use a different language (or format). \n",
    "\n",
    "+ **Do not just send your code. The homework solutions should be in a report style. Be sure to add comments to your code as well as markdown cells where you describe your approach and discuss your results. **\n",
    "\n",
    "+ Please submit your notebook in an executed status, so that we can see all the results you computed. However, we will still run your code and all cells should reproduce the output when executed. \n",
    "\n",
    "+ If you have multiple files (e.g. you've added code files or images) create a tarball for all files in a single file and name it: AM207_YOURNAME_HW5.tar.gz or AM207_YOURNAME_HW5.zip\n",
    "\n",
    "\n",
    "### Have Fun!\n",
    "_ _ _ _ _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import scipy.stats \n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: HMM... I Think Your Text Got Corrupted!\n",
    "\n",
    "In this problem you should use a Hidden Markov Model to correct typos in a text without using a dictionary. Your data is in two different text files:\n",
    "\n",
    "* `Shakespeare_correct.txt` contains the words of some sonnets from Shakespeare\n",
    "* `Shakespeare_typos.txt` contains the same text, but now some of the characters are corrupted\n",
    "\n",
    "For convenience both text files only contain lower case letters a-z and spaces. \n",
    "\n",
    "First build a first order HMM:\n",
    "* What are the hidden states and what are the observed states?\n",
    "* What should you do to generate your HMM probability matrices?\n",
    "* For some of the HMM parameters, you won't have enough training data to get representative probabilities.  For example, some of your probabilites might be 0. You should address this problem by adding a small pseudocount, similar to the motif finding problem from a previous assignment. \n",
    "* Implement the Viterbi algorithm and run it on a test portion that contains errors. Show that your Viterbi implementation can improve text of length 100, 500, 1000, and 2000. Note: To do this correctly you would have to withhold the part of the text that you use for testing when you estimate the parameters for you HMM. For the sake of this homework it is ok though to report training error instead of test error. Just be aware that the correction rate you are reporting most likely is a very optimistic estimate. \n",
    "* What correction rate do you get?\n",
    "\n",
    "**Important**: Wikipedia has a nice article on [Viterbi](https://en.wikipedia.org/wiki/Viterbi_algorithm). **Please do not use the python implementation from this article!** (The lecture notebook also has the version from Wikipedia). Using dictionaries for Viterbi is really not intuitive and using numpy is typically faster. The article has very nice pseudo code that should enable you to easily program Viterbi by yourself. Please also refrain for this problem from using any other third party implementations. \n",
    "\n",
    "Now for a second order HMM:\n",
    "By using a second order HMM, you should be able to get a better correction rate. \n",
    "* Give an intuitive explanation why a second order HMM should give better results.\n",
    "* Implement your second order text correction. Hint: If you think a bit about the model you won't even have to change your Viterbi implementation. \n",
    "* Compare your correction rates against the first order model for text length of 100 and 500, (you can do 1000 as well if your computer is fast enough). \n",
    "* How well would your implementation scale to HMMs of even higher order? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hidden States: The true characters\n",
    "### Observed States: The characters in Shakespeare_typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Order:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The HMM probability matrices are 27 by 27 matrices.  \n",
    "### In the emission Matrix, each row represents the observable characters (26 letters plus space), and each column represents the true characters (same possible range as the observable characters).  \n",
    "### The transition matrix has starting characters for each row, and following characters for each column.  To construct the transition matrix, I go through the correct text and count how many times each character follows each character.  This can then be normalized row-wise to get the probability of having each character follow the character that is represented by that row.  \n",
    "### The transition matrix has hidden states (characters) as the rows, and observed states as the columns.  To construct the emission matrix, I go through both lines of text and count how many times each character is present in the typo text given the true character.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['from', 'fairest', 'creatures', 'we', 'desire', 'increase', 'that',\n",
       "       'thereby', 'beautys', 'rose'], \n",
       "      dtype='|S16')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_correct[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put data into strings\n",
    "\n",
    "with open('Shakespeare_correct.txt') as file:\n",
    "    s_correct = file.read()\n",
    "    \n",
    "with open('Shakespeare_typos.txt') as file:\n",
    "    s_typo = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A dictionary to relate characters to indices\n",
    "char_dict = {}\n",
    "for i,char in enumerate(string.lowercase+' '):\n",
    "    char_dict.update({char:i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create emission matrix\n",
    "\n",
    "emission = np.zeros((27,27))\n",
    "for i in range(len(s_correct)):\n",
    "    # Find characters\n",
    "    char_correct = s_correct[i]\n",
    "    char_typo = s_typo[i]\n",
    "    \n",
    "    # Add a count to the emission matrix at the corresponding position\n",
    "    emission[char_dict[char_correct],char_dict[char_typo]] += 1\n",
    "    \n",
    "# Normalize the rows\n",
    "for i in range(emission.shape[0]):\n",
    "    row = emission[i,:]\n",
    "    emission[i,:] = row/np.sum(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create transition matrix\n",
    "\n",
    "transition = np.zeros((27,27))\n",
    "for i in range(len(s_correct)-1):\n",
    "    # Get the character and the one that follows it\n",
    "    first_char = s_correct[i]\n",
    "    next_char = s_correct[i+1]\n",
    "    \n",
    "    # Add a count to the transition matrix for each combination\n",
    "    transition[char_dict[first_char],char_dict[next_char]] += 1\n",
    "    \n",
    "# Normalize the rows\n",
    "for i in range(transition.shape[0]):\n",
    "    row = transition[i,:]\n",
    "    transition[i,:] = row/np.sum(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create starting probabilities.  This could be done a few ways, but what I \n",
    "#    implement below is the probability of each letter being the initial \n",
    "#    character in a word in the true text, since the first character in the\n",
    "#    typo text is the beginning of a word.  \n",
    "\n",
    "start_probs = np.zeros(27)\n",
    "\n",
    "# Split the words\n",
    "words = s_correct.split(' ')\n",
    "\n",
    "for word in words:\n",
    "    first_char = word[0]\n",
    "    start_probs[char_dict[first_char]] += 1\n",
    "    \n",
    "# Normalize\n",
    "start_probs = start_probs/np.sum(start_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I create another dictionary to match characters to indices:\n",
    "index_dict = {}\n",
    "\n",
    "for i,char in enumerate(string.lowercase+' '):\n",
    "    index_dict.update({i:char})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(obs, start_pr=start_probs, em=emission, tr=transition, \\\n",
    "            e_pseudo = 0.000001, t_pseudo = 0.000001, p_pseudo=0.000001):\n",
    "    # add pseudocounts to make all elements non-zero\n",
    "    e = em + e_pseudo\n",
    "    t = tr + t_pseudo\n",
    "    start_p = start_pr + p_pseudo\n",
    "    \n",
    "    # Array of probabilities for each character at each step in the document\n",
    "    viterbi_probs = np.zeros((27,len(obs)))\n",
    "\n",
    "    # Initial state:\n",
    "    first_char = obs[0]\n",
    "    # I use log probabilities to avoid underflow\n",
    "    viterbi_probs[:,0] = np.log(start_p) + np.log(e[char_dict[first_char],:])\n",
    "\n",
    "    for i in range(1,len(obs)):\n",
    "        # At each step, find the character and use it to index the appropriate matrix entries\n",
    "        char = obs[i]\n",
    "        char_idx = char_dict[char]\n",
    "        \n",
    "        for j in range(len(char_dict)):\n",
    "            # Find the max of the log probabilities for all possible characters\n",
    "            prob = max([(viterbi_probs[k,i-1] + np.log(t[k,j]) + np.log(e[j,char_idx]))\\\n",
    "                       for k in range(len(char_dict))])\n",
    "            viterbi_probs[j,i] = prob\n",
    "            \n",
    "    # Predictions are the max probability for each step\n",
    "    char_predictions = np.argmax(viterbi_probs,axis=0)\n",
    "    \n",
    "    # Translate the max probabilities back to characters\n",
    "    s_viterbi = ''\n",
    "    for i in char_predictions:\n",
    "        s_viterbi += index_dict[i]\n",
    "        \n",
    "    return s_viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I create a function to calculate the accuracy of a corrupted text:\n",
    "def accuracy(corrupt, correct):\n",
    "    # count of total errors\n",
    "    errors = 0.\n",
    "    for i in range(len(corrupt)):\n",
    "        if corrupt[i] != correct[i]:\n",
    "            errors += 1.\n",
    "    return 1. - errors/len(correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall accuracy of the typo text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8374634600053149"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(s_typo,s_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below I define a method to test the algorithm on a random slice of a specified length.  To get a better idea of how the algorithm performs in general, I will run many samples of a fixed length and return accuracies with errors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I create a function to test the algorithm on random slices of a specified length:\n",
    "def test_viterbi(length):\n",
    "    # Random starting position\n",
    "    start = np.random.randint(low=0, high = len(s_correct)-length)\n",
    "    \n",
    "    # Pick out the random segment:\n",
    "    typo_segment = s_typo[start:start+length]\n",
    "    correct_segment = s_correct[start:start+length]\n",
    "    \n",
    "    # Run viterbi:\n",
    "    corrected_segment = viterbi(typo_segment)\n",
    "    \n",
    "    # Return accuracy of the original segment and the corrected segment\n",
    "    return accuracy(typo_segment, correct_segment), accuracy(corrected_segment, correct_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_sample(size,samples):\n",
    "    print \"For text of length {}, based on {} samples:\".format(size,samples)\n",
    "    samples = np.zeros((samples,2))\n",
    "    for i in range(samples.shape[0]):\n",
    "        orig, corrected = test_viterbi(size)\n",
    "        samples[i,0] = orig\n",
    "        samples[i,1] = corrected\n",
    "    print \"Original document accuracy: {:.3f} +/- {:.3f}\".format(np.mean(samples[:,0]),np.std(samples[:,0]))\n",
    "    print \"Viterbi-corrected accuracy: {:.3f} +/- {:.3f}\".format(np.mean(samples[:,1]),np.std(samples[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text of length 100, based on 30 samples:\n",
      "Original document accuracy: 0.836 +/- 0.036\n",
      "Viterbi-corrected accuracy: 0.919 +/- 0.037\n",
      "\n",
      "For text of length 500, based on 30 samples:\n",
      "Original document accuracy: 0.834 +/- 0.019\n",
      "Viterbi-corrected accuracy: 0.908 +/- 0.014\n",
      "\n",
      "For text of length 1000, based on 30 samples:\n",
      "Original document accuracy: 0.838 +/- 0.009\n",
      "Viterbi-corrected accuracy: 0.913 +/- 0.010\n",
      "\n",
      "For text of length 2000, based on 30 samples:\n",
      "Original document accuracy: 0.837 +/- 0.006\n",
      "Viterbi-corrected accuracy: 0.909 +/- 0.007\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [100, 500, 1000, 2000]:\n",
    "    test_sample(i,30)\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I see that the accuracy of the original typo samples was consistent for various lengths.  My Viterbi implementation improved the accuracy for all of the lengths tested, and this improvement did not depend on the length of the text.  The precision of the estimates of the accuracy, however, improved as test length was increased.  For a text size of 2000 characters, the estimates are fairly precise, and there is clear evidence that the Viterbi algorithm is improving the accuracy of the text.  \n",
    "### I did see that the improvement is sensitive to the pseudocounts, and making these very small causes certain states to have extremely low probabilities.  Tuning the values of the pseudocounts may lead to further improvements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second Order: \n",
    "\n",
    "### Intuitively, the second order HMM should give further improvements because it incorporates more information.  It is reasonable to believe that each hidden state does not depend solely on the state before it, rather it could be modeled to depend on the previous state and the state before that.  That is, there is a structure in words in which the a given pair of letters is not independent on the previous letter, so we should be able to see more improvement with a 2nd order model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I chose a different implementation than what I saw people discussing on Piazza.  I started the assignment late due to other assignment, job interviews, etc, so I didn't have a chance to discuss my implementation with anyone, but I think that the following should be valid.  \n",
    "\n",
    "### Rather than considering pairs of characters explicitly, I created two transition matrices.  \n",
    "### The first is the same one used above, and represents the probability of transitioning to one hidden state from the previous step.  \n",
    "### The second is similar, and represents the probability of transitioning to one hidden state from the state two steps previous.  \n",
    "### Both of these matrices are 27 by 27 because they measure the probability of transitioning from a single character to a single character, and my emission matrix is the original 27 by 27 matrix.  \n",
    "### Now in each iteration, I consider the transition from the previous step to the current step as well as the transition from 2 steps previos to the current step, and I add both log-probabilities.  \n",
    "### These two transition probabilities should be weighted, and these weights add a hyperparameter to tune.  Due to time limitations and the extremely long runtime added by the additional 'for' statement to consider the two step-prior transition, I did not experiment with the weights thoroughly, and mostly stuck with 80% importance placed on the one-step-prior transition and 20% importance placed on the two-step-prior transition.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transition matrix for one character to the position two spaces ahead of it\n",
    "transition2 = np.zeros((27,27))\n",
    "for i in range(len(s_correct)-2):\n",
    "    # Get the character pair and the character that follows it\n",
    "    two_back = s_correct[i]\n",
    "    next_char = s_correct[i+2]\n",
    "    \n",
    "    # Add a count to the transition matrix for each combination\n",
    "    transition2[char_dict[two_back],char_dict[next_char]] += 1\n",
    "    \n",
    "# Normalize the rows\n",
    "for i in range(transition2.shape[0]):\n",
    "    row = transition2[i,:]\n",
    "    transition2[i,:] = row/(np.sum(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With this in place, the modifications to the earlier code are minimal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi2(obs, start_pr=start_probs, em=emission, tr=transition, tr2=transition2,\\\n",
    "            e_pseudo = 0.000001, t_pseudo = 0.000001, p_pseudo=0.000001):\n",
    "    ########### I have added the second transition matrix to the list of arguments\n",
    "    \n",
    "    # add pseudocounts to make all elements non-zero\n",
    "    e = em + e_pseudo\n",
    "    t = tr + t_pseudo\n",
    "    t2 = tr + t_pseudo ####### add pseudocounts as to the rest\n",
    "    start_p = start_pr + p_pseudo\n",
    "    \n",
    "    # Array of probabilities for each character at each step in the document\n",
    "    viterbi_probs = np.zeros((27,len(obs)))\n",
    "\n",
    "    # Initial state:\n",
    "    first_char = obs[0]\n",
    "    # I use log probabilities to avoid underflow\n",
    "    viterbi_probs[:,0] = np.log(start_p) + np.log(e[char_dict[first_char],:])\n",
    "    \n",
    "    ###### Now I also need to initialize the second step.  I do so as I would have performed the first\n",
    "    ######    iteration of the last implementation.  \n",
    "    char = obs[1]\n",
    "    char_idx = char_dict[char]\n",
    "    for j in range(len(char_dict)):\n",
    "        prob = max(viterbi_probs[k,0] + np.log(t[k,j]) + np.log(e[j,char_idx]) for k in range(len(char_dict)))\n",
    "        viterbi_probs[j,1] = prob\n",
    "\n",
    "    ##### The rest starts at the third step, and each result depends on the previous 2.  \n",
    "    for i in range(2,len(obs)):\n",
    "        \n",
    "        # At each step, find the character and use it to index the appropriate matrix entries\n",
    "        char = obs[i]\n",
    "        char_idx = char_dict[char]\n",
    "        \n",
    "        for j in range(len(char_dict)):\n",
    "            # Find the max of the log probabilities for all possible characters\n",
    "            \n",
    "            ###### In viterbi 2, I add the log-probability of transitioning to this state from the state\n",
    "            ######     two steps previous.  This adds another 'for loop' and slows the code considerably.  \n",
    "            ######\n",
    "            ###### Here, I weight the importance of each step at 80%/20%.\n",
    "            ######\n",
    "            ###### This is my favorite pair of percentages, but it would be great to experiment with \n",
    "            ######     different weightings to test the importance of the character two steps prior.  \n",
    "            ######\n",
    "            \n",
    "            prob = max([(0.8 * viterbi_probs[k,i-1] + 0.8 * np.log(t[k,j]) + \\\n",
    "                         0.2 * viterbi_probs[k2,i-2] + 0.2 * np.log(t2[k2,j]) +\\\n",
    "                         + np.log(e[j,char_idx])) for k in range(len(char_dict)) for k2 in range(len(char_dict))])\n",
    "            viterbi_probs[j,i] = prob\n",
    "            \n",
    "    # Predictions are the max probability for each step\n",
    "    char_predictions = np.argmax(viterbi_probs,axis=0)\n",
    "    \n",
    "    # Translate the max probabilities back to characters\n",
    "    s_viterbi = ''\n",
    "    for i in char_predictions:\n",
    "        s_viterbi += index_dict[i]\n",
    "        \n",
    "    return s_viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I create a function to test the algorithm on random slices of a specified length:\n",
    "######## Modified minimally from above.\n",
    "\n",
    "def test_viterbi2(length):\n",
    "    # Random starting position\n",
    "    start = np.random.randint(low=0, high = len(s_correct)-length)\n",
    "    \n",
    "    # Pick out the random segment:\n",
    "    typo_segment = s_typo[start:start+length]\n",
    "    correct_segment = s_correct[start:start+length]\n",
    "    \n",
    "    # Run viterbi:\n",
    "    corrected_segment = viterbi2(typo_segment)\n",
    "    \n",
    "    # Return accuracy of the original segment and the corrected segment\n",
    "    return accuracy(typo_segment, correct_segment), accuracy(corrected_segment, correct_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_sample2(size,samples):\n",
    "    print \"For text of length {}, based on {} samples:\".format(size,samples)\n",
    "    samples = np.zeros((samples,2))\n",
    "    for i in range(samples.shape[0]):\n",
    "        orig, corrected = test_viterbi2(size)  ####### The only change from above\n",
    "        samples[i,0] = orig\n",
    "        samples[i,1] = corrected\n",
    "    print \"Original document accuracy: {:.3f} +/- {:.3f}\".format(np.mean(samples[:,0]),np.std(samples[:,0]))\n",
    "    print \"Viterbi-corrected accuracy: {:.3f} +/- {:.3f}\".format(np.mean(samples[:,1]),np.std(samples[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text of length 100, based on 25 samples:\n",
      "Original document accuracy: 0.849 +/- 0.029\n",
      "Viterbi-corrected accuracy: 0.904 +/- 0.030\n",
      "CPU times: user 3min 18s, sys: 2.01 s, total: 3min 20s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_sample2(100,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text of length 500, based on 25 samples:\n",
      "Original document accuracy: 0.836 +/- 0.014\n",
      "Viterbi-corrected accuracy: 0.902 +/- 0.015\n",
      "CPU times: user 17min 28s, sys: 11.1 s, total: 17min 39s\n",
      "Wall time: 17min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_sample2(500,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For text of length 1000, based on 25 samples:\n",
      "Original document accuracy: 0.838 +/- 0.009\n",
      "Viterbi-corrected accuracy: 0.900 +/- 0.011\n",
      "CPU times: user 33min 41s, sys: 21.3 s, total: 34min 3s\n",
      "Wall time: 34min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_sample2(1000,25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_sample2(2000,25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I ran out of time to run all of the tests, but my initial results with the 2nd order HMM were not promising.  The results were comparable to the 1st order model.  \n",
    "#### With more time, I would experiment with the pseudocounts and the weights of 1-step and 2-step to see if these could improve the accuracy of the corrected text.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Problem 2: Final Project Review\n",
    "    \n",
    "You will be contacted shortly by a TF to meet and discuss your final project proposal. Be sure to take advantage of this feedback option. Review meetings should be scheduled within the week from April 11-15. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We spoke with Xide on 4/15/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
